{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbaf316",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "本期我们将学习二分类-软输出的一种算法：逻辑回归。该算法主要是依托于一个激活函数：$sigmoid$，因为这个函数的值域为$(0, 1)$，因此可以近似表示概率值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847cdb19",
   "metadata": {},
   "source": [
    "## Origin\n",
    "以下为 $shuhuai$老师的讲义上的解释：\n",
    ">有时候我们只要得到一个类别的概率，那么我们需要一种能输出$(0, 1)$区间的值的函数。考虑两分类模型，我们利用判别模型，希望对$p(C|x)$建模，利用贝叶斯定理：\n",
    "$$\n",
    "p\\left(C_{1} \\mid x\\right)=\\frac{p\\left(x \\mid C_{1}\\right) p\\left(C_{1}\\right)}{p\\left(x \\mid C_{1}\\right) p\\left(C_{1}\\right)+p\\left(x \\mid C_{2}\\right) p\\left(C_{2}\\right)}\n",
    "$$\n",
    "取 $a=\\ln \\frac{p\\left(x \\mid C_{1}\\right) p\\left(C_{1}\\right)}{p\\left(x \\mid C_{2}\\right) p\\left(C_{2}\\right)}$, 于是：\n",
    "$$\n",
    "p\\left(C_{1} \\mid x\\right)=\\frac{1}{1+\\exp (-a)}\n",
    "$$\n",
    "上面的式子叫$Logistic\\ Sigmoid$函数，其参数表示了两类联合概率比值的对数。在判别式中，不关心这个参数的具体值，模型假设直接对 $a$ 进行。\n",
    "\n",
    "当然了，老师高端的解释看不懂也没关系，我们只需要知道，现在我们有了一个激活函数 $sigmoid$，它可以用来得到一个类别的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf80ff",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa513ec1",
   "metadata": {},
   "source": [
    "首先我们给出逻辑回归的模型假设：\n",
    "$$\n",
    "f(x)=\\sigma(w^Tx)\n",
    "$$\n",
    "其中，$\\sigma(a)=sigmoid(a)$，我们一般用 $\\sigma$ 来表示激活函数\n",
    "\n",
    "于是，通过寻找 $w$ 的最佳值，则可以确定在该模型假设下的最佳模型。\n",
    "\n",
    "概率判别模型常用极大似然估计来确定参数。\n",
    "\n",
    "为了确定似然函数，我们先做一些标记：\n",
    "$$\n",
    "p_1=\\sigma(w^Tx) \\quad p_0=1-p_1\n",
    "$$\n",
    "其中 $p_1$ 为 $x$ 属于$1$类的概率，$p_0$ 为 $x$ 属于$0$类的概率\n",
    "\n",
    "下面我们就可以给出该模型的似然函数了：\n",
    "$$\n",
    "p(y|w;x)=p_1^yp_0^{1-y}\n",
    "$$\n",
    "这个似然函数看上去操作有点骚，看不懂，其实也蛮合理的：\n",
    "* 当$y$为$1$时：$p(y|w;x)=p_1^1p_0^0=p_1$\n",
    "* 当$y$为$0$时：$p(y|w;x)=p_1^0p_0^1=p_0$\n",
    "\n",
    "好，下面我们就可以使用极大似然估计来确定参数了\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\hat{w}=argmax(J(w))&=argmax(p(Y|w;X))\\\\\n",
    "&=argmax(log(p(Y|w;X)))\\\\\n",
    "&=argmax(log(\\prod_{i=1}^n p(y_i|w;x_i)))\\\\\n",
    "&=argmax(\\sum_{i=1}^n log(p(y_i)|w;x_i))\\\\\n",
    "&=argmax(\\sum_{i=1}^n y\\ log\\, p_1+(1-y)log\\,p_0)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "注意到，这个表达式是交叉熵表达式的相反数乘 $N,MLE$ 中的对数也保证了可以和指数函数相匹配, 从而在大的区间汇总获取稳定的梯度。\n",
    "\n",
    "对上式求导，我们注意到：\n",
    "$$\n",
    "p_1'=p_1(1-p_1)\n",
    "$$\n",
    "当然这个也很容易得到，就是链式法则嘛，稍微细心一点就可以求出来了。\n",
    "\n",
    "最后我们求出结果：\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w}J(w)=\\sum_{i=1}^{N}\\left(y_{i}-p_{1}\\right) x_{i}\n",
    "$$\n",
    "最后还有一点要注意，我们是要求得$p(p|w;x)$的最大值，因此我们需要使用梯度上升，而不是梯度下降，当然两者也差不多，加个负号而已。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3a9a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.34954661]\n",
      " [-0.50817047]\n",
      " [ 3.07719068]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "from models.linear_models import Logistic_regression\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "epsilon = 1\n",
    "num_test = 100\n",
    "num_base = 1000\n",
    "ratio = 0.6\n",
    "k1, k2 = 3, 5\n",
    "b1, b2 = 1, 2\n",
    "X = np.linspace(0, 100, num_base)\n",
    "X_train = X[:-num_test]\n",
    "X_test = X[-num_test:]\n",
    "v1 = X_train[:round(len(X_train) * ratio)] * k1 + b1\n",
    "v2 = X_train[round(len(X_train) * ratio):] * k2 + b2\n",
    "v1 += np.random.normal(scale=epsilon, size=v1.shape)\n",
    "v2 += np.random.normal(scale=epsilon, size=v2.shape)\n",
    "value = np.r_[v1, v2]\n",
    "data = np.c_[X_train, value]\n",
    "l1 = np.ones_like(v1)\n",
    "l2 = np.zeros_like(v2)\n",
    "label = np.r_[l1, l2]\n",
    "v_test_c1 = X_test * k1 + b1\n",
    "l_test_c1 = np.ones_like(v_test_c1)\n",
    "data_test = np.c_[X_test, v_test_c1]\n",
    "\n",
    "model = Logistic_regression(10, 1000, lr=1e-3)\n",
    "model.fit(data, label)\n",
    "print(model.get_params())\n",
    "print(model.predict(data_test, l_test_c1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
