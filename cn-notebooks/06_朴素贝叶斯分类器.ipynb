{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f29c85",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "本期我们学习线性分类-软输出-概率生成模型的另一种算法：朴素贝叶斯假设。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090cfdb",
   "metadata": {},
   "source": [
    "## Idea\n",
    "上一期我们学习的高斯判别分析是对数据集总体做出了高斯分布的假设，同时引入伯努利分布作为标签的先验，从而利用最大后验估计求得假设的参数。\n",
    "\n",
    "而本期我们学习的朴素贝叶斯假设则是对数据的属性之间的关系做出了假设：条件独立性假设。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef8ee7",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097aee9",
   "metadata": {},
   "source": [
    "一般情况下，我们要得到 $p(x|y)$ 这个概率，由于 $x$ 有 $p$ 个维度，因此需要对这 $p$ 个随机变量组成的联合分布进行采样，但我们知道：对于如此高维度的空间，需要采集极其庞大数量的样本才能获得较为准确的概率近似。\n",
    "\n",
    "在一般的有向概率图模型中，通常对各个属性维度之间的条件独立关系做出了不同的假设，其中最为简单的假设就是在朴素贝叶斯模型中描述的条件独立性假设：\n",
    "$$\n",
    "p(x|y)=\\prod_{i=1}^p p(x_i|y)p(y)\n",
    "$$\n",
    "用数学语言来描述：\n",
    "$$\n",
    "x_{i} \\perp x_{j} | y, \\forall i \\neq j\n",
    "$$\n",
    "利用贝叶斯定理，对于单次观测：\n",
    "$$\n",
    "p(y|x)=\\frac{p(x|y)p(y)}{p(x)}=\\frac{\\prod_{i=1}^p p(x_i|y)p(y)}{p(x)}\n",
    "$$\n",
    "与高斯判别分析类似，下面对数据的分布做出一些假设：\n",
    "* $x_i$为离散变量:\n",
    "   * 一般设$x_i$服从类别分布(Categorical)：$p(x_i=i|y)=\\theta_i,\\sum_{i=1}^p \\theta_i =1$\n",
    "\n",
    "* $x_i$为连续变量：\n",
    "   * 一般设 $x_i$ 服从高斯分布：$p(x_i|y)=N(\\mu_i, \\Sigma_i)$\n",
    "* 二分类：\n",
    "    * $y \\sim Bernoulli(\\phi):p(y)=\\phi^y (1-\\phi)^{(1-y)}$ \n",
    "* 多分类：\n",
    "    * $y \\sim Categorical\\ Dist\\quad p(y_i)=\\theta_i\\quad \\sum_{i=1}^k \\theta_i=1$\n",
    "\n",
    "对于这些参数的估计，一般可以直接通过对数据集的采样来估计。参数估计好后，预测时代入贝叶斯定理求出后验概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bb557c",
   "metadata": {},
   "source": [
    "## Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6fedb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([6.763511927008758, 0.0676351192700876], [4.499499499499499, 1.44994994994995], [6.763511927008758, 0.6087160734307883], [4.499499499499499, 3.34984984984985])\n",
      "accuary: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "from models.linear_models import NaiveBayesClassifier\n",
    "\n",
    "num_test = 100\n",
    "x = np.linspace(0, 10, 1000)\n",
    "k1, k2 = 0.1, 0.3\n",
    "b1, b2 = 1, 2\n",
    "x_train = x[:-num_test]\n",
    "x_test = x[-num_test:]\n",
    "v_1 = x_train * k1 + b1\n",
    "v_2 = x_train * k2 + b2\n",
    "train_data = np.r_[np.c_[x_train, v_1], np.c_[x_train, v_2]]\n",
    "train_label = np.r_[np.ones_like(x_train), np.zeros_like(x_train)]\n",
    "\n",
    "model = NaiveBayesClassifier()\n",
    "model.fit(train_data, train_label)\n",
    "print(model.get_params())\n",
    "\n",
    "v_test = x_test * k2 + b2\n",
    "data_test = np.c_[x_test, v_test]\n",
    "print(\"accuary:\", model.predict(data_test, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
